{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification of gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all file names in a given directory\n",
    "def list_files(folder):\n",
    "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
    "    return textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 75\n",
      "25 50\n"
     ]
    }
   ],
   "source": [
    "f_files = list_files(\"celebs-gb/female\")\n",
    "m_files = list_files(\"celebs-gb/male\")\n",
    "X = f_files + m_files #X is usually used to denote the dataset to be trained and tested on, i.e. the features (or where features are extracted from)\n",
    "y = [\"female\"] * len(f_files) + [\"male\"] * len(m_files) #y is usually used to store the labels/classes. Here we simply repeat female for how many female users we have, and then the same for male. Obviously X and y must be in same order.\n",
    "\n",
    "print(len(X), len(y))\n",
    "print(y.count(\"female\"), y.count(\"male\")) # more males than females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 15\n",
      "60 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 40\n",
      "5 10\n"
     ]
    }
   ],
   "source": [
    "print(y_train.count(\"female\"), y_train.count(\"male\"))\n",
    "print(y_test.count(\"female\"), y_test.count(\"male\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename', analyzer='word') # text files read in and text extracted (default = extract words (word tokenise) and count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform` both creates the vocabulary from the training data (fit), and creates a vector for each training instance (document), which in this case will be the counts for each word in the vocabulary. Because no restriction has been set on the vocabulary, every word type found in the training set will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 174103)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '30am',\n",
       " '6kgwu66sif',\n",
       " '_landonmoss',\n",
       " 'angelskiss',\n",
       " 'bbcfrontrow',\n",
       " 'brexitwipe',\n",
       " 'charger',\n",
       " 'crept',\n",
       " 'dg9kvdnbnm',\n",
       " 'eaza5vhrdy',\n",
       " 'ezrdicv0q1',\n",
       " 'fsfefyqqmd',\n",
       " 'gphxrpkk3k',\n",
       " 'hhrl1zr76t',\n",
       " 'ilovemaysunico',\n",
       " 'jeby6gcaml',\n",
       " 'kate_murphy_',\n",
       " 'lasty',\n",
       " 'lvornfjif0',\n",
       " 'michelle_dyer',\n",
       " 'na5fevgawe',\n",
       " 'o5dco4fbly',\n",
       " 'parisestcharlie',\n",
       " 'prime',\n",
       " 'rabbijoshy',\n",
       " 'romilly',\n",
       " 'sepia',\n",
       " 'springsteen',\n",
       " 'tbfdajen',\n",
       " 'toward',\n",
       " 'upotzq63h0',\n",
       " 'w29jmfjeg5',\n",
       " 'ww90ggeifw',\n",
       " 'youknowwhatitis']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[::5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear', random_state=0)\n",
    "clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female']\n"
     ]
    }
   ],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "predictions = clf.predict(X_test_vectorized)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.50      0.60      0.55         5\n",
      "        male       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.64      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.67        15\n",
      "\n",
      "[[3 2]\n",
      " [3 7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['female', 'male'], dtype='<U6')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>celebs-gb/male/TomFletcher.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>celebs-gb/male/AndreWinner.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>celebs-gb/male/CraigyFerg.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celebs-gb/male/IanJamesPoulter.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>celebs-gb/female/natashabdnfield.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>celebs-gb/female/KellyOsbourne.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>celebs-gb/male/OzzyOsbourne.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>celebs-gb/male/AndersFoghR.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>celebs-gb/male/DerrenBrown.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>celebs-gb/male/mrjimsturgess.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>celebs-gb/male/Robin_Leach.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>celebs-gb/male/neilhimself.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>celebs-gb/female/Marsha_Thomason.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>celebs-gb/female/lilyallen.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>celebs-gb/female/EstelleDarlings.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Instance  Actual Predicted  Correct\n",
       "0         celebs-gb/male/TomFletcher.txt    male      male     True\n",
       "1         celebs-gb/male/AndreWinner.txt    male      male     True\n",
       "2          celebs-gb/male/CraigyFerg.txt    male      male     True\n",
       "3     celebs-gb/male/IanJamesPoulter.txt    male      male     True\n",
       "4   celebs-gb/female/natashabdnfield.txt  female    female     True\n",
       "5     celebs-gb/female/KellyOsbourne.txt  female    female     True\n",
       "6        celebs-gb/male/OzzyOsbourne.txt    male      male     True\n",
       "7         celebs-gb/male/AndersFoghR.txt    male    female    False\n",
       "8         celebs-gb/male/DerrenBrown.txt    male    female    False\n",
       "9       celebs-gb/male/mrjimsturgess.txt    male      male     True\n",
       "10        celebs-gb/male/Robin_Leach.txt    male    female    False\n",
       "11        celebs-gb/male/neilhimself.txt    male      male     True\n",
       "12  celebs-gb/female/Marsha_Thomason.txt  female      male    False\n",
       "13        celebs-gb/female/lilyallen.txt  female      male    False\n",
       "14  celebs-gb/female/EstelleDarlings.txt  female    female     True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(X_test,y_test,predictions,y_test==predictions)), columns=[\"Instance\", \"Actual\", \"Predicted\",\"Correct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test) # performs transforms on already fitted steps before the classifier, then finally predict on last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.50      0.60      0.55         5\n",
      "        male       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.64      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.67        15\n",
      "\n",
      "[[3 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=1000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(vectorizer__max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.60      0.60      0.60         5\n",
      "        male       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.70      0.70      0.70        15\n",
      "weighted avg       0.73      0.73      0.73        15\n",
      "\n",
      "[[3 2]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=1000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model.set_params(clf=MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.75      0.60      0.67         5\n",
      "        male       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.78      0.75      0.76        15\n",
      "weighted avg       0.80      0.80      0.79        15\n",
      "\n",
      "[[3 2]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "cv_scores = cross_validate(model, X, y, \n",
    "                           cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                           return_train_score=False, \n",
    "                           scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([2.35361218, 2.3269248 , 2.24833107, 2.38351417, 2.662956  ]), 'score_time': array([0.4209559 , 0.43346024, 0.3942821 , 0.5183208 , 0.37261701]), 'test_accuracy': array([0.53333333, 0.6       , 0.73333333, 0.86666667, 0.73333333]), 'test_precision_weighted': array([0.55555556, 0.6       , 0.77380952, 0.86666667, 0.73333333]), 'test_recall_weighted': array([0.53333333, 0.6       , 0.73333333, 0.86666667, 0.73333333]), 'test_f1_weighted': array([0.54226475, 0.6       , 0.74074074, 0.86666667, 0.73333333])}\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_scores_summary(name, scores):\n",
    "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 69.33%, sd = 11.62%, min = 53.33, max = 86.67\n",
      "Precision: mean = 70.59%, sd = 11.40%, min = 55.56, max = 86.67\n",
      "Recall: mean = 69.33%, sd = 11.62%, min = 53.33, max = 86.67\n",
      "F1: mean = 69.66%, sd = 11.43%, min = 54.23, max = 86.67\n"
     ]
    }
   ],
   "source": [
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 73.33%, sd = 11.16%, min = 60.00, max = 86.67\n",
      "Precision: mean = 73.95%, sd = 12.19%, min = 55.56, max = 88.89\n",
      "Recall: mean = 73.33%, sd = 11.16%, min = 60.00, max = 86.67\n",
      "F1: mean = 72.66%, sd = 11.46%, min = 56.82, max = 85.61\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', Binarizer()),\n",
    "    ('norm2', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 69.33%, sd = 6.80%, min = 60.00, max = 80.00\n",
      "Precision: mean = 65.80%, sd = 13.88%, min = 42.86, max = 80.95\n",
      "Recall: mean = 69.33%, sd = 6.80%, min = 60.00, max = 80.00\n",
      "F1: mean = 63.89%, sd = 9.48%, min = 50.00, max = 79.37\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove features with low variance across instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word')\n",
    "selector = SelectKBest(chi2, k=100)\n",
    "feats = vectorizer.fit_transform(X_train)\n",
    "filtered = selector.fit_transform(feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "cols = selector.get_support()\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "print(list(compress(names,cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(score_func = chi2)),\n",
    "    ('clf', None), # clf set in param_grid.\n",
    "])\n",
    "\n",
    "search = GridSearchCV(model, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 500],\n",
    "                          'clf': [MultinomialNB(), LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      },\n",
    "                      verbose=2)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous code used sklearn's tokenisation and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def custom_tokenise(text):\n",
    "    return tokenise_re.findall(text.lower())\n",
    "\n",
    "def nltk_twitter_tokenise(text):\n",
    "    twtok = nltk.tokenize.TweetTokenizer()\n",
    "    return twtok.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word', tokenizer=custom_tokenise, preprocessor=preprocess)\n",
    "feats = vectorizer.fit_transform(X_train)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[::2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
