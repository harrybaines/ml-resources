{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification of gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all file names in a given directory\n",
    "def list_files(folder):\n",
    "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
    "    return textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 75\n",
      "25 50\n"
     ]
    }
   ],
   "source": [
    "f_files = list_files(\"celebs-gb/female\")\n",
    "m_files = list_files(\"celebs-gb/male\")\n",
    "X = f_files + m_files #X is usually used to denote the dataset to be trained and tested on, i.e. the features (or where features are extracted from)\n",
    "y = [\"female\"] * len(f_files) + [\"male\"] * len(m_files) #y is usually used to store the labels/classes. Here we simply repeat female for how many female users we have, and then the same for male. Obviously X and y must be in same order.\n",
    "\n",
    "print(len(X), len(y))\n",
    "print(y.count(\"female\"), y.count(\"male\")) # more males than females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 15\n",
      "60 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 40\n",
      "5 10\n"
     ]
    }
   ],
   "source": [
    "print(y_train.count(\"female\"), y_train.count(\"male\"))\n",
    "print(y_test.count(\"female\"), y_test.count(\"male\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename', analyzer='word') # text files read in and text extracted (default = extract words (word tokenise) and count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform` both creates the vocabulary from the training data (fit), and creates a vector for each training instance (document), which in this case will be the counts for each word in the vocabulary. Because no restriction has been set on the vocabulary, every word type found in the training set will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 174103)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '30am',\n",
       " '6kgwu66sif',\n",
       " '_landonmoss',\n",
       " 'angelskiss',\n",
       " 'bbcfrontrow',\n",
       " 'brexitwipe',\n",
       " 'charger',\n",
       " 'crept',\n",
       " 'dg9kvdnbnm',\n",
       " 'eaza5vhrdy',\n",
       " 'ezrdicv0q1',\n",
       " 'fsfefyqqmd',\n",
       " 'gphxrpkk3k',\n",
       " 'hhrl1zr76t',\n",
       " 'ilovemaysunico',\n",
       " 'jeby6gcaml',\n",
       " 'kate_murphy_',\n",
       " 'lasty',\n",
       " 'lvornfjif0',\n",
       " 'michelle_dyer',\n",
       " 'na5fevgawe',\n",
       " 'o5dco4fbly',\n",
       " 'parisestcharlie',\n",
       " 'prime',\n",
       " 'rabbijoshy',\n",
       " 'romilly',\n",
       " 'sepia',\n",
       " 'springsteen',\n",
       " 'tbfdajen',\n",
       " 'toward',\n",
       " 'upotzq63h0',\n",
       " 'w29jmfjeg5',\n",
       " 'ww90ggeifw',\n",
       " 'youknowwhatitis']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[::5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear', random_state=0)\n",
    "clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'female'\n",
      " 'male' 'female' 'male' 'male' 'male' 'female']\n"
     ]
    }
   ],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "predictions = clf.predict(X_test_vectorized)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.50      0.60      0.55         5\n",
      "        male       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.64      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.67        15\n",
      "\n",
      "[[3 2]\n",
      " [3 7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['female', 'male'], dtype='<U6')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>celebs-gb/male/TomFletcher.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>celebs-gb/male/AndreWinner.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>celebs-gb/male/CraigyFerg.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celebs-gb/male/IanJamesPoulter.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>celebs-gb/female/natashabdnfield.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>celebs-gb/female/KellyOsbourne.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>celebs-gb/male/OzzyOsbourne.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>celebs-gb/male/AndersFoghR.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>celebs-gb/male/DerrenBrown.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>celebs-gb/male/mrjimsturgess.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>celebs-gb/male/Robin_Leach.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>celebs-gb/male/neilhimself.txt</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>celebs-gb/female/Marsha_Thomason.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>celebs-gb/female/lilyallen.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>celebs-gb/female/EstelleDarlings.txt</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Instance  Actual Predicted  Correct\n",
       "0         celebs-gb/male/TomFletcher.txt    male      male     True\n",
       "1         celebs-gb/male/AndreWinner.txt    male      male     True\n",
       "2          celebs-gb/male/CraigyFerg.txt    male      male     True\n",
       "3     celebs-gb/male/IanJamesPoulter.txt    male      male     True\n",
       "4   celebs-gb/female/natashabdnfield.txt  female    female     True\n",
       "5     celebs-gb/female/KellyOsbourne.txt  female    female     True\n",
       "6        celebs-gb/male/OzzyOsbourne.txt    male      male     True\n",
       "7         celebs-gb/male/AndersFoghR.txt    male    female    False\n",
       "8         celebs-gb/male/DerrenBrown.txt    male    female    False\n",
       "9       celebs-gb/male/mrjimsturgess.txt    male      male     True\n",
       "10        celebs-gb/male/Robin_Leach.txt    male    female    False\n",
       "11        celebs-gb/male/neilhimself.txt    male      male     True\n",
       "12  celebs-gb/female/Marsha_Thomason.txt  female      male    False\n",
       "13        celebs-gb/female/lilyallen.txt  female      male    False\n",
       "14  celebs-gb/female/EstelleDarlings.txt  female    female     True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(X_test,y_test,predictions,y_test==predictions)), columns=[\"Instance\", \"Actual\", \"Predicted\",\"Correct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test) # performs transforms on already fitted steps before the classifier, then finally predict on last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.50      0.60      0.55         5\n",
      "        male       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.64      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.67        15\n",
      "\n",
      "[[3 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=1000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(vectorizer__max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.60      0.60      0.60         5\n",
      "        male       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.70      0.70      0.70        15\n",
      "weighted avg       0.73      0.73      0.73        15\n",
      "\n",
      "[[3 2]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='filename', lowercase=True, max_df=1.0,\n",
       "                                 max_features=1000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model.set_params(clf=MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.75      0.60      0.67         5\n",
      "        male       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.78      0.75      0.76        15\n",
      "weighted avg       0.80      0.80      0.79        15\n",
      "\n",
      "[[3 2]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "cv_scores = cross_validate(model, X, y, \n",
    "                           cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                           return_train_score=False, \n",
    "                           scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([2.35361218, 2.3269248 , 2.24833107, 2.38351417, 2.662956  ]), 'score_time': array([0.4209559 , 0.43346024, 0.3942821 , 0.5183208 , 0.37261701]), 'test_accuracy': array([0.53333333, 0.6       , 0.73333333, 0.86666667, 0.73333333]), 'test_precision_weighted': array([0.55555556, 0.6       , 0.77380952, 0.86666667, 0.73333333]), 'test_recall_weighted': array([0.53333333, 0.6       , 0.73333333, 0.86666667, 0.73333333]), 'test_f1_weighted': array([0.54226475, 0.6       , 0.74074074, 0.86666667, 0.73333333])}\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_scores_summary(name, scores):\n",
    "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 69.33%, sd = 11.62%, min = 53.33, max = 86.67\n",
      "Precision: mean = 70.59%, sd = 11.40%, min = 55.56, max = 86.67\n",
      "Recall: mean = 69.33%, sd = 11.62%, min = 53.33, max = 86.67\n",
      "F1: mean = 69.66%, sd = 11.43%, min = 54.23, max = 86.67\n"
     ]
    }
   ],
   "source": [
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 73.33%, sd = 11.16%, min = 60.00, max = 86.67\n",
      "Precision: mean = 73.95%, sd = 12.19%, min = 55.56, max = 88.89\n",
      "Recall: mean = 73.33%, sd = 11.16%, min = 60.00, max = 86.67\n",
      "F1: mean = 72.66%, sd = 11.46%, min = 56.82, max = 85.61\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', Binarizer()),\n",
    "    ('norm2', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 69.33%, sd = 6.80%, min = 60.00, max = 80.00\n",
      "Precision: mean = 65.80%, sd = 13.88%, min = 42.86, max = 80.95\n",
      "Recall: mean = 69.33%, sd = 6.80%, min = 60.00, max = 80.00\n",
      "F1: mean = 63.89%, sd = 9.48%, min = 50.00, max = 79.37\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove features with low variance across instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 76.00%, sd = 12.36%, min = 60.00, max = 93.33\n",
      "Precision: mean = 76.15%, sd = 14.05%, min = 55.56, max = 94.44\n",
      "Recall: mean = 76.00%, sd = 12.36%, min = 60.00, max = 93.33\n",
      "F1: mean = 75.30%, sd = 13.00%, min = 56.82, max = 93.46\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word')\n",
    "selector = SelectKBest(chi2, k=100)\n",
    "feats = vectorizer.fit_transform(X_train)\n",
    "filtered = selector.fit_transform(feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaw', 'ableton', 'aleshaofficial', 'alexandrapark1', 'and', 'awesome', 'bass', 'bcacampaign', 'beats1', 'bgt', 'bit', 'campaign', 'cet', 'children', 'cnni', 'collection', 'connect', 'discover', 'dj', 'djrap', 'dress', 'dripmas', 'education', 'elizabethhurleybeach', 'f8ltezveav', 'fab', 'falabella', 'fb', 'for', 'former', 'gorgeous', 'heap', 'heforshe', 'her', 'https', 'in', 'interview', 'iran', 'itvthismorning', 'jxdmijgtab', 'lionsgatetv', 'love', 'lovelies', 'lovely', 'luv', 'ly', 'me', 'mimu_gloves', 'much', 'music', 'my', 'myceliaworldtour', 'new', 'noushskaugen', 'onlyconnectquiz', 'our', 'p91rbigbtg', 'paolo', 'paul', 'pbs', 'popnow', 'pres', 'president', 'propa', 'rap', 'repost', 'robbie', 'rw', 'says', 'school', 'shop', 'so', 'soulsville', 'stella', 'stella_kids', 'stellamccartney', 'stellamenswear', 'stellasworld', 'summer', 'syria', 'syrian', 'tells', 'thank', 'thebodyguarduk', 'theirworld', 'theroyals', 'theroyalsone', 'thrive', 'tickets', 'to', 'twitpic', 'upforschool', 'with', 'women', 'xx', 'xxx', 'xxxx', 'xxxxx', 'you', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "cols = selector.get_support()\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "print(list(compress(names,cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10, total=   2.6s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10, total=   2.9s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10, total=   3.3s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10, total=   2.5s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=10, total=   2.5s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50, total=   2.7s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50, total=   2.8s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50, total=   2.3s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50, total=   2.7s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=50, total=   3.0s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100, total=   2.8s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100, total=   2.7s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100, total=   2.6s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100, total=   2.9s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=100, total=   3.0s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500, total=   3.0s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500, total=   2.7s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500, total=   2.8s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500, total=   2.7s\n",
      "[CV] clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500 \n",
      "[CV]  clf=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), selector__k=500, total=   2.7s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10, total=   3.1s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10, total=   3.0s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10, total=   2.8s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10, total=   2.7s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=10, total=   2.8s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50, total=   2.5s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50, total=   2.5s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=50, total=   2.5s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=100, total=   2.4s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500, total=   2.3s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500, total=   2.6s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500, total=   2.5s\n",
      "[CV] clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500 \n",
      "[CV]  clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), selector__k=500, total=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='filename',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocesso...\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l2',\n",
       "                                                    random_state=0,\n",
       "                                                    solver='liblinear',\n",
       "                                                    tol=0.0001, verbose=0,\n",
       "                                                    warm_start=False)],\n",
       "                         'selector__k': [10, 50, 100, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit='f1_weighted',\n",
       "             return_train_score=False,\n",
       "             scoring=['accuracy', 'precision_weighted', 'recall_weighted',\n",
       "                      'f1_weighted'],\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(score_func = chi2)),\n",
    "    ('clf', None), # clf set in param_grid.\n",
    "])\n",
    "\n",
    "search = GridSearchCV(model, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 500],\n",
    "                          'clf': [MultinomialNB(), LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      },\n",
    "                      verbose=2)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_selector__k</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>split0_test_f1_weighted</th>\n",
       "      <th>split1_test_f1_weighted</th>\n",
       "      <th>split2_test_f1_weighted</th>\n",
       "      <th>split3_test_f1_weighted</th>\n",
       "      <th>split4_test_f1_weighted</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>std_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.380322</td>\n",
       "      <td>0.263452</td>\n",
       "      <td>0.390582</td>\n",
       "      <td>0.079884</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf': MultinomialNB(alpha=1.0, class_prior=N...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>5</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.592075</td>\n",
       "      <td>0.564341</td>\n",
       "      <td>0.039790</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.306577</td>\n",
       "      <td>0.227543</td>\n",
       "      <td>0.378461</td>\n",
       "      <td>0.078398</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>50</td>\n",
       "      <td>{'clf': MultinomialNB(alpha=1.0, class_prior=N...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084984</td>\n",
       "      <td>7</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.561365</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.409195</td>\n",
       "      <td>0.174696</td>\n",
       "      <td>0.396301</td>\n",
       "      <td>0.061039</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf': MultinomialNB(alpha=1.0, class_prior=N...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097183</td>\n",
       "      <td>4</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.584639</td>\n",
       "      <td>0.120936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.367837</td>\n",
       "      <td>0.123183</td>\n",
       "      <td>0.404287</td>\n",
       "      <td>0.031605</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf': MultinomialNB(alpha=1.0, class_prior=N...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.121708</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.473850</td>\n",
       "      <td>0.156286</td>\n",
       "      <td>0.419622</td>\n",
       "      <td>0.053897</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062361</td>\n",
       "      <td>2</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.656515</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.073931</td>\n",
       "      <td>0.133268</td>\n",
       "      <td>0.342176</td>\n",
       "      <td>0.053779</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>50</td>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.565826</td>\n",
       "      <td>0.553906</td>\n",
       "      <td>0.072798</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.971467</td>\n",
       "      <td>0.070197</td>\n",
       "      <td>0.339679</td>\n",
       "      <td>0.054523</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143372</td>\n",
       "      <td>2</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.913165</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.156376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.072761</td>\n",
       "      <td>0.115575</td>\n",
       "      <td>0.336124</td>\n",
       "      <td>0.058472</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135401</td>\n",
       "      <td>6</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.621973</td>\n",
       "      <td>0.145425</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.380322      0.263452         0.390582        0.079884   \n",
       "1       2.306577      0.227543         0.378461        0.078398   \n",
       "2       2.409195      0.174696         0.396301        0.061039   \n",
       "3       2.367837      0.123183         0.404287        0.031605   \n",
       "4       2.473850      0.156286         0.419622        0.053897   \n",
       "5       2.073931      0.133268         0.342176        0.053779   \n",
       "6       1.971467      0.070197         0.339679        0.054523   \n",
       "7       2.072761      0.115575         0.336124        0.058472   \n",
       "\n",
       "                                           param_clf param_selector__k  \\\n",
       "0  MultinomialNB(alpha=1.0, class_prior=None, fit...                10   \n",
       "1  MultinomialNB(alpha=1.0, class_prior=None, fit...                50   \n",
       "2  MultinomialNB(alpha=1.0, class_prior=None, fit...               100   \n",
       "3  MultinomialNB(alpha=1.0, class_prior=None, fit...               500   \n",
       "4  LogisticRegression(C=1.0, class_weight=None, d...                10   \n",
       "5  LogisticRegression(C=1.0, class_weight=None, d...                50   \n",
       "6  LogisticRegression(C=1.0, class_weight=None, d...               100   \n",
       "7  LogisticRegression(C=1.0, class_weight=None, d...               500   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'clf': MultinomialNB(alpha=1.0, class_prior=N...              0.666667   \n",
       "1  {'clf': MultinomialNB(alpha=1.0, class_prior=N...              0.666667   \n",
       "2  {'clf': MultinomialNB(alpha=1.0, class_prior=N...              0.666667   \n",
       "3  {'clf': MultinomialNB(alpha=1.0, class_prior=N...              0.666667   \n",
       "4  {'clf': LogisticRegression(C=1.0, class_weight...              0.750000   \n",
       "5  {'clf': LogisticRegression(C=1.0, class_weight...              0.500000   \n",
       "6  {'clf': LogisticRegression(C=1.0, class_weight...              0.583333   \n",
       "7  {'clf': LogisticRegression(C=1.0, class_weight...              0.583333   \n",
       "\n",
       "   split1_test_accuracy  split2_test_accuracy  ...  std_test_recall_weighted  \\\n",
       "0              0.666667              0.666667  ...                  0.033333   \n",
       "1              0.750000              0.583333  ...                  0.084984   \n",
       "2              0.833333              0.583333  ...                  0.097183   \n",
       "3              0.833333              0.583333  ...                  0.084984   \n",
       "4              0.583333              0.666667  ...                  0.062361   \n",
       "5              0.666667              0.500000  ...                  0.074536   \n",
       "6              0.750000              0.916667  ...                  0.143372   \n",
       "7              0.750000              0.833333  ...                  0.135401   \n",
       "\n",
       "   rank_test_recall_weighted  split0_test_f1_weighted  \\\n",
       "0                          5                 0.533333   \n",
       "1                          7                 0.629630   \n",
       "2                          4                 0.533333   \n",
       "3                          1                 0.533333   \n",
       "4                          2                 0.739496   \n",
       "5                          8                 0.500000   \n",
       "6                          2                 0.592593   \n",
       "7                          6                 0.592593   \n",
       "\n",
       "   split1_test_f1_weighted  split2_test_f1_weighted  split3_test_f1_weighted  \\\n",
       "0                 0.629630                 0.533333                 0.533333   \n",
       "1                 0.694737                 0.491228                 0.491228   \n",
       "2                 0.814815                 0.491228                 0.491228   \n",
       "3                 0.814815                 0.491228                 0.629630   \n",
       "4                 0.491228                 0.629630                 0.666667   \n",
       "5                 0.629630                 0.444444                 0.629630   \n",
       "6                 0.739496                 0.913165                 0.629630   \n",
       "7                 0.739496                 0.833333                 0.500000   \n",
       "\n",
       "   split4_test_f1_weighted  mean_test_f1_weighted  std_test_f1_weighted  \\\n",
       "0                 0.592075               0.564341              0.039790   \n",
       "1                 0.500000               0.561365              0.084914   \n",
       "2                 0.592593               0.584639              0.120936   \n",
       "3                 0.739496               0.641700              0.121708   \n",
       "4                 0.755556               0.656515              0.094700   \n",
       "5                 0.565826               0.553906              0.072798   \n",
       "6                 0.444444               0.663866              0.156376   \n",
       "7                 0.444444               0.621973              0.145425   \n",
       "\n",
       "   rank_test_f1_weighted  \n",
       "0                      6  \n",
       "1                      7  \n",
       "2                      5  \n",
       "3                      3  \n",
       "4                      2  \n",
       "5                      8  \n",
       "6                      1  \n",
       "7                      4  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.67      0.40      0.50         5\n",
      "        male       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.71      0.65      0.66        15\n",
      "weighted avg       0.72      0.73      0.71        15\n",
      "\n",
      "[[2 3]\n",
      " [1 9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous code used sklearn's tokenisation and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def custom_tokenise(text):\n",
    "    return tokenise_re.findall(text.lower())\n",
    "\n",
    "def nltk_twitter_tokenise(text):\n",
    "    twtok = nltk.tokenize.TweetTokenizer()\n",
    "    return twtok.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 65687)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word', tokenizer=custom_tokenise, preprocessor=preprocess)\n",
    "feats = vectorizer.fit_transform(X_train)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " \"'too\",\n",
       " '4x100m',\n",
       " 'amatra',\n",
       " 'battentastic',\n",
       " 'bruise',\n",
       " \"cleese's\",\n",
       " 'cv',\n",
       " 'downsides',\n",
       " 'extremists',\n",
       " 'galeries',\n",
       " 'harshly',\n",
       " 'indictments',\n",
       " 'kitzi',\n",
       " 'mackerel',\n",
       " 'monkeying',\n",
       " \"of'\",\n",
       " 'pierce',\n",
       " 'racially',\n",
       " 'roughing',\n",
       " 'shutdown',\n",
       " 'stoke',\n",
       " 'thunk',\n",
       " 'untangle',\n",
       " 'wit',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[::2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean = 66.67%, sd = 13.33%, min = 60.00, max = 93.33\n",
      "Precision: mean = 69.36%, sd = 12.40%, min = 60.00, max = 93.94\n",
      "Recall: mean = 66.67%, sd = 13.33%, min = 60.00, max = 93.33\n",
      "F1: mean = 67.29%, sd = 12.92%, min = 60.00, max = 93.12\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
